{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR on 360 Images — Hands-On Demo\n",
    "\n",
    "In this notebook, we'll run text recognition on a panoramic image using **[PanoOCR](https://github.com/yz3440/panoocr)** and explore the results.\n",
    "\n",
    "PanoOCR handles the hard part automatically:\n",
    "1. Splits the equirectangular panorama into overlapping perspective views\n",
    "2. Runs an OCR engine on each view\n",
    "3. Converts results to spherical coordinates (yaw/pitch)\n",
    "4. Deduplicates overlapping detections\n",
    "\n",
    "**Documentation:** [yz3440.github.io/panoocr](https://yz3440.github.io/panoocr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install PanoOCR\n",
    "\n",
    "Pick the right engine for your platform:\n",
    "- **macOS**: `panoocr[macocr]` — uses Apple Vision Framework (fast, accurate)\n",
    "- **Windows/Linux**: `panoocr[paddleocr]` — uses PaddleOCR\n",
    "\n",
    "Uncomment the line for your platform below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macOS (Apple Vision Framework)\n",
    "!pip install \"panoocr[macocr]\"\n",
    "\n",
    "# Windows / Linux (PaddleOCR)\n",
    "# !pip install \"panoocr[paddleocr]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load a Sample Panorama\n",
    "\n",
    "Place an equirectangular panorama in `assets/sample_panorama.jpg`, or use the cell below to download one from Google Street View using the `streetlevel` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SAMPLE_IMAGE = \"assets/sample_panorama.jpg\"\n",
    "\n",
    "# If no sample image exists, download one from Google Street View\n",
    "if not os.path.exists(SAMPLE_IMAGE):\n",
    "    print(\"No sample image found. Downloading from Google Street View...\")\n",
    "    from streetlevel import streetview\n",
    "    \n",
    "    # Near MIT Media Lab — a spot with plenty of visible text\n",
    "    pano = streetview.find_panorama(42.3625, -71.0862)\n",
    "    if pano:\n",
    "        os.makedirs(\"assets\", exist_ok=True)\n",
    "        streetview.download_panorama(pano, SAMPLE_IMAGE, zoom=4)\n",
    "        print(f\"Downloaded panorama {pano.id} ({pano.date})\")\n",
    "    else:\n",
    "        print(\"Could not find a panorama. Please place an image at assets/sample_panorama.jpg\")\n",
    "\n",
    "# Display the panorama\n",
    "img = Image.open(SAMPLE_IMAGE)\n",
    "print(f\"Image size: {img.size[0]} x {img.size[1]}\")\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Sample Panorama (equirectangular)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set Up the OCR Engine\n",
    "\n",
    "Choose the engine for your platform. The cell below defaults to MacOCR — change it if you're on Windows/Linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    # macOS — Apple Vision Framework\n",
    "    from panoocr.engines.macocr import MacOCREngine\n",
    "    engine = MacOCREngine()\n",
    "    print(\"Using MacOCR (Apple Vision Framework)\")\n",
    "else:\n",
    "    # Windows / Linux — PaddleOCR\n",
    "    from panoocr.engines.paddleocr import PaddleOCREngine\n",
    "    engine = PaddleOCREngine()\n",
    "    print(\"Using PaddleOCR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run OCR on the Panorama\n",
    "\n",
    "This is the main step. PanoOCR will:\n",
    "- Generate perspective views from the equirectangular image\n",
    "- Run the OCR engine on each view\n",
    "- Convert and deduplicate results\n",
    "\n",
    "This may take a minute depending on image size and your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panoocr import PanoOCR\n",
    "\n",
    "pano_ocr = PanoOCR(engine)\n",
    "result = pano_ocr.recognize(SAMPLE_IMAGE)\n",
    "\n",
    "print(f\"Found {len(result.results)} text detections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explore the Results\n",
    "\n",
    "Each result has:\n",
    "- `text` — the recognized text\n",
    "- `yaw` — horizontal position in degrees (-180 to 180)\n",
    "- `pitch` — vertical position in degrees (-90 to 90)\n",
    "- `confidence` — OCR confidence score\n",
    "- `width`, `height` — angular size in degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 20 results sorted by confidence\n",
    "sorted_results = sorted(result.results, key=lambda r: r.confidence, reverse=True)\n",
    "\n",
    "print(f\"{'Text':<30} {'Yaw':>6} {'Pitch':>6} {'Conf':>5}\")\n",
    "print(\"-\" * 52)\n",
    "for r in sorted_results[:20]:\n",
    "    text_display = r.text[:28] + \"..\" if len(r.text) > 30 else r.text\n",
    "    print(f\"{text_display:<30} {r.yaw:>6.1f} {r.pitch:>6.1f} {r.confidence:>5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results as JSON\n",
    "\n",
    "Save the results for use with the interactive 3D preview tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"assets/ocr_results.json\"\n",
    "result.save_json(output_path)\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize: Result Positions on the Panorama\n",
    "\n",
    "Let's plot where each text detection sits on the panorama using yaw/pitch coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "yaws = [r.yaw for r in result.results]\n",
    "pitches = [r.pitch for r in result.results]\n",
    "confs = [r.confidence for r in result.results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.imshow(img, extent=[-180, 180, -90, 90], aspect='auto', alpha=0.5)\n",
    "scatter = ax.scatter(yaws, pitches, c=confs, cmap='viridis', s=20, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Confidence')\n",
    "ax.set_xlabel('Yaw (degrees)')\n",
    "ax.set_ylabel('Pitch (degrees)')\n",
    "ax.set_title('OCR Detections on Panorama')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive 3D Preview\n",
    "\n",
    "For a much better visualization, use PanoOCR's interactive 3D preview tool:\n",
    "\n",
    "```bash\n",
    "# Clone panoocr and start a local server\n",
    "git clone https://github.com/yz3440/panoocr.git\n",
    "cd panoocr/preview\n",
    "python -m http.server 8000\n",
    "```\n",
    "\n",
    "Then open `http://localhost:8000` in your browser and drag in:\n",
    "1. The panorama image (`assets/sample_panorama.jpg`)\n",
    "2. The JSON results file (`assets/ocr_results.json`)\n",
    "\n",
    "You'll see the OCR results positioned on an interactive 3D sphere that you can rotate and zoom."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
